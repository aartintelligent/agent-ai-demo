# ğŸ§  Agent AI Demo

Ce projet est une dÃ©monstration technique visant Ã  mettre en valeur mes compÃ©tences en dÃ©veloppement dâ€™un agent conversationnel intelligent, conÃ§u sur mesure en Rust.

Il sâ€™agit dâ€™un agent autonome, capable de :

GÃ©rer son propre Ã©tat interne via une machine Ã  Ã©tats typÃ©e (ChatAgentStateMachine)

RÃ©agir dynamiquement Ã  des requÃªtes utilisateurs, en sÃ©lectionnant des outils spÃ©cialisÃ©s (ex: recherche arXiv)

IntÃ©grer des appels externes Ã  des services ou sources de donnÃ©es (API, scraping, etc.)

Structurer ses sorties de maniÃ¨re claire et naturelle

## Objectif

### Recherche dâ€™articles scientifiques via arXiv

Ce projet intÃ¨gre un composant permettant dâ€™interroger automatiquement la plateforme [arXiv.org](https://arxiv.org) afin
de rÃ©cupÃ©rer des articles scientifiques pertinents en rÃ©ponse Ã  une requÃªte utilisateur.

#### Fonctionnement

* Lorsquâ€™une requÃªte de type recherche scientifique est dÃ©tectÃ©e, le systÃ¨me construit dynamiquement une requÃªte HTTP
  vers lâ€™**API arXiv**.
* Le flux de rÃ©ponse, au format **Atom XML**, est parsÃ© et transformÃ© en une structure Rust typÃ©e (`ArxivResult`),
  contenant :

    * le titre de lâ€™article,
    * un rÃ©sumÃ© (abstract),
    * un lien vers la publication.

#### Exemple de scÃ©nario

> Utilisateur : *"Quels sont les derniers travaux sur les agents conversationnels multi-modaux ?"*

Lâ€™agent effectue alors une recherche sur arXiv avec le mot-clÃ© `"multi-modal conversational agents"` et retourne une
sÃ©lection dâ€™articles pertinents.

#### Format de rÃ©ponse

Le systÃ¨me retourne les rÃ©sultats sous forme synthÃ©tique, en langage naturel, avec un lien cliquable vers chaque
article, exemple : Titre, RÃ©sumÃ© et lien.

#### Remarques

* Seuls les 5 Ã  10 premiers rÃ©sultats sont retournÃ©s, filtrÃ©s par pertinence.
* Lâ€™API arXiv est publique et ne nÃ©cessite pas dâ€™authentification, mais respecte une limite de taux implicite (\~1
  req/s).
* Lâ€™intÃ©gration actuelle est focalisÃ©e sur la **catÃ©gorie IA (cs.AI)** mais peut Ãªtre Ã©tendue Ã  d'autres domaines (
  `stat.ML`, `cs.CL`, etc.).

## Execution

[![Demo vidÃ©o](https://img.youtube.com/vi/IkI-7Fj_CJ4/hqdefault.jpg)](https://www.youtube.com/watch?v=IkI-7Fj_CJ4)

**OpenIA + gpt-4o-mini**

=== Research Assistant State Machine Demo ===

ğŸ” Searching arXiv for 'multi-modal conversational agents'

Processing result 1...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

Le document intitulÃ© "DAG-Net: Double Attentive Graph Neural Network for Trajectory Forecasting" propose un modÃ¨le visant Ã  comprendre le comportement humain en matiÃ¨re de mouvement, essentiel pour des applications telles que les voitures autonomes ou les robots sociaux. Ce dÃ©fi est complexe en raison de la nature multi-modale du mouvement humain, oÃ¹ plusieurs trajectoires futures sont possibles Ã  partir des chemins historiques. En outre, les activitÃ©s humaines sont souvent orientÃ©es par des objectifs, comme atteindre des lieux spÃ©cifiques ou interagir avec l'environnement.

Pour rÃ©pondre Ã  ces enjeux, les auteurs prÃ©sentent un nouveau modÃ¨le gÃ©nÃ©ratif rÃ©current qui prend en compte les objectifs futurs des agents individuels ainsi que les interactions entre diffÃ©rents agents. Ce modÃ¨le utilise un rÃ©seau de neurones graphiques basÃ© sur une double attention pour recueillir des informations sur les influences mutuelles entre agents et intÃ©grer ces donnÃ©es avec les objectifs futurs. Ce modÃ¨le est suffisamment gÃ©nÃ©ral pour Ãªtre appliquÃ© Ã  divers scÃ©narios et atteint des rÃ©sultats de pointe tant dans les environnements urbains que dans des applications sportives. 

Pour plus de dÃ©tails, vous pouvez consulter le lien : [DAG-Net sur arXiv](http://arxiv.org/abs/2005.12661v2).

ğŸ“ State: Ready

Processing result 2...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

Le document intitulÃ© "Using Conversational Agents To Support Learning By Teaching" explore comment les agents conversationnels peuvent Ãªtre utilisÃ©s pour soutenir le paradigme "apprendre en enseignant". Traditionnellement, les agents pÃ©dagogiques sont conÃ§us pour jouer le rÃ´le d'enseignants humains en donnant des instructions aux Ã©lÃ¨ves. Cependant, cette Ã©tude se concentre sur l'utilisation d'agents conversationnels qui reÃ§oivent des instructions de la part des Ã©tudiants.

Les auteurs prÃ©sentent une application Ã©ducative nommÃ©e Curiosity Notebook, qui exploite les interventions conversationnelles pour faciliter l'apprentissage des Ã©tudiants. La reconnaissance de ces interventions peut non seulement aider Ã  engager les Ã©tudiants lors des interactions d'apprentissage, mais aussi offrir une comprÃ©hension plus approfondie des dÃ©fis liÃ©s Ã  la conception d'agents conversationnels Ã  des fins Ã©ducatives.

Pour plus d'informations, vous pouvez consulter le lien : [Using Conversational Agents To Support Learning By Teaching sur arXiv](http://arxiv.org/abs/1909.13443v1).

ğŸ“ State: Ready

Processing result 3...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

Le document intitulÃ© "Designing Style Matching Conversational Agents" aborde les avancÃ©es en intelligence artificielle qui permettent de crÃ©er des interfaces conversationnelles susceptibles de transformer radicalement les interactions humaines avec les machines. MalgrÃ© ces progrÃ¨s, des lacunes subsistent dans la capacitÃ© des agents Ã  interagir de maniÃ¨re naturelle, notamment parce qu'ils adoptent souvent un comportement monotone et ne s'adaptent pas Ã  leur interlocuteur.

Les auteurs prÃ©sentent deux agents conversationnels de bout-en-bout : le premier est un agent vocal capable de mener des dialogues naturels et de s'aligner sur le style de conversation de l'interlocuteur, tandis que le second, un agent conversationnel incarnÃ© (ECA), reconnaÃ®t le comportement humain lors de conversations ouvertes et ajuste automatiquement ses rÃ©ponses en fonction du style visuel et conversationnel de l'autre participant. L'ECA utilise des entrÃ©es multimodales pour gÃ©nÃ©rer des rÃ©ponses vocales et faciales riches et perceptuellement valables, telles que le synchronisme labial et les expressions faciales.

Ã€ partir des rÃ©sultats empiriques d'Ã©tudes utilisateurs, les auteurs soulignent plusieurs dÃ©fis majeurs dans la construction de tels systÃ¨mes et proposent des directives de conception pour les interactions de dialogue multi-tours en utilisant l'adaptation de style dans la recherche future.

Pour plus de dÃ©tails, vous pouvez consulter le lien : [Designing Style Matching Conversational Agents sur arXiv](http://arxiv.org/abs/1910.07514v1).

ğŸ“ State: Ready

Processing result 4...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

Le document intitulÃ© "What Makes a Good Conversation? Challenges in Designing Truly Conversational Agents" aborde les limites des agents conversationnels, qui promettent une interaction conversationnelle mais Ã©chouent souvent Ã  livrer cette expÃ©rience. Les tentatives de crÃ©ation de ces agents se basent souvent sur des rÃ¨gles fonctionnelles issues de la communication humaine, sans prendre en compte les caractÃ©ristiques clÃ©s que doit contenir une vÃ©ritable conversation.

L'objectif de l'Ã©tude est de comprendre ce que les gens apprÃ©cient dans une conversation et comment cela devrait se reflÃ©ter dans la conception des agents. Les rÃ©sultats d'entretiens semi-structurÃ©s montrent que les participants Ã©tablissent une distinction claire entre les rÃ´les sociaux et fonctionnels des conversations, soulignant l'importance des dynamiques de lien et de confiance Ã  long terme, ainsi que l'importance du contexte et de la phase de la relation dans le type de conversations qu'ils entretiennent.

Les participants remettent Ã©galement en question la nÃ©cessitÃ© d'un lien et d'un terrain d'entente dans la communication avec les agents, se tournant plutÃ´t vers des dÃ©finitions plus utilitaires des qualitÃ©s conversationnelles. Sur la base de ces rÃ©sultats, les auteurs discutent des dÃ©fis majeurs liÃ©s Ã  la conception d'agents conversationnels, notamment la nÃ©cessitÃ© de redÃ©finir les paramÃ¨tres de conception pour les interactions avec ces agents.

Pour plus de dÃ©tails, vous pouvez consulter le lien : [What Makes a Good Conversation? Challenges in Designing Truly Conversational Agents sur arXiv](http://arxiv.org/abs/1901.06525v1).

ğŸ“ State: Ready

Processing result 5...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

Le document intitulÃ© "Memory Sandbox: Transparent and Interactive Memory Management for Conversational Agents" traite de la gestion de la mÃ©moire dans les agents conversationnels alimentÃ©s par des modÃ¨les de langage de grande taille (LLM), comme chatGPT. Pour offrir des rÃ©ponses contextuellement pertinentes, ces agents doivent se souvenir d'informations clÃ©s d'une conversation en cours. Cependant, leur mÃ©moire est limitÃ©e et ils peuvent Ãªtre distraits par des Ã©lÃ©ments non pertinents de la conversation.

Actuellement, les utilisateurs n'ont pas de moyens efficaces pour visualiser et contrÃ´ler ce que l'agent se souvient, ce qui entraÃ®ne une mauvaise comprÃ©hension de la dynamique de la conversation et des interruptions dans l'interaction. Les auteurs introduisent "Memory Sandbox", un systÃ¨me interactif qui permet aux utilisateurs de gÃ©rer la mÃ©moire conversationnelle des agents alimentÃ©s par LLM. En traitant les souvenirs comme des objets de donnÃ©es pouvant Ãªtre visualisÃ©s, manipulÃ©s, enregistrÃ©s, rÃ©sumÃ©s et partagÃ©s Ã  travers les conversations, Memory Sandbox offre des fonctionnalitÃ©s d'interaction pour que les utilisateurs puissent guider la perception de la conversation par l'agent.

Pour plus de dÃ©tails, vous pouvez consulter le lien : [Memory Sandbox sur arXiv](http://arxiv.org/abs/2308.01542v1).

ğŸ“ State: Ready

=== Demo Complete ===

---

**Ollama + mistral-small3.2**

=== Research Assistant State Machine Demo ===

ğŸ” Searching arXiv for 'llm agents'

Processing result 1...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

**RÃ©sumÃ© du document :**

L'article intitulÃ© "A Survey of Large Language Model Agents for Question Answering" examine les progrÃ¨s rÃ©alisÃ©s dans le domaine des agents basÃ©s sur de grands modÃ¨les de langage (LLM) pour les systÃ¨mes de rÃ©ponse aux questions (QA). Les agents traditionnels prÃ©sentent des limites importantes, telles que des besoins en donnÃ©es substantiels et des difficultÃ©s Ã  gÃ©nÃ©raliser Ã  de nouveaux environnements. Les agents basÃ©s sur les LLM surmontent ces dÃ©fis en utilisant les LLM comme moteur de raisonnement central. Ces agents obtiennent de meilleurs rÃ©sultats en QA par rapport aux pipelines traditionnels et aux systÃ¨mes simples de QA basÃ©s sur les LLM, grÃ¢ce Ã  leur capacitÃ© d'interaction avec des environnements externes.

L'article passe en revue systÃ©matiquement la conception des agents LLM dans le contexte des tÃ¢ches de QA, en organisant la discussion autour des Ã©tapes clÃ©s suivantes : planification, comprÃ©hension des questions, rÃ©cupÃ©ration d'informations et gÃ©nÃ©ration de rÃ©ponses. De plus, l'article identifie les dÃ©fis actuels et explore les directions de recherche futures pour amÃ©liorer les performances des systÃ¨mes de QA basÃ©s sur les agents LLM.

**Lien vers l'article :** [http://arxiv.org/abs/2503.19213v1](http://arxiv.org/abs/2503.19213v1)

ğŸ“ State: Ready

Processing result 2...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

**RÃ©sumÃ© du document :**

L'article intitulÃ© "Can LLMs Understand Social Norms in Autonomous Driving Games?" explore l'application des grands modÃ¨les de langage (LLM) pour comprendre et modÃ©liser les normes sociales dans les jeux de conduite autonome. Les normes sociales sont dÃ©finies comme des standards partagÃ©s de comportement acceptable dans une sociÃ©tÃ©, et leur Ã©mergence favorise la coordination entre les agents sans rÃ¨gles codÃ©es en dur, ce qui est crucial pour le dÃ©ploiement Ã  grande Ã©chelle des vÃ©hicules autonomes (AV) dans les systÃ¨mes de transport intelligents.

Les auteurs introduisent des agents basÃ©s sur les LLM dans des jeux de conduite autonome, oÃ¹ ces agents prennent des dÃ©cisions en fonction de prompts textuels. Le cadre de travail implique des agents basÃ©s sur les LLM jouant des jeux de Markov dans un systÃ¨me multi-agents (MAS), permettant d'Ã©tudier l'Ã©mergence des normes sociales parmi les agents individuels. Les auteurs visent Ã  identifier les normes sociales en concevant des prompts et en utilisant les LLM sur des informations textuelles liÃ©es Ã  la configuration de l'environnement et aux observations des agents basÃ©s sur les LLM.

Les expÃ©riences sont menÃ©es en utilisant l'API OpenAI Chat alimentÃ©e par GPT-4.0 pour simuler des interactions et Ã©valuer les performances des agents basÃ©s sur les LLM dans deux scÃ©narios de conduite : une intersection non signalisÃ©e et un peloton autoroutier. Les rÃ©sultats montrent que les agents basÃ©s sur les LLM peuvent gÃ©rer des environnements dynamiquement changeants dans les jeux de Markov, et que les normes sociales Ã©voluent parmi les agents basÃ©s sur les LLM dans les deux scÃ©narios. Dans le jeu d'intersection, les agents basÃ©s sur les LLM tendent Ã  adopter une politique de conduite conservative lorsqu'ils sont confrontÃ©s Ã  un potentiel accident de voiture. L'avantage des agents basÃ©s sur les LLM dans les jeux rÃ©side dans leur forte opÃ©rabilitÃ© et analysabilitÃ©, ce qui facilite la conception expÃ©rimentale.

**Lien vers l'article :** [http://arxiv.org/abs/2408.12680v2](http://arxiv.org/abs/2408.12680v2)

ğŸ“ State: Ready

Processing result 3...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

**RÃ©sumÃ© du document :**

L'article intitulÃ© "LLM Agent Honeypot: Monitoring AI Hacking Agents in the Wild" aborde la menace croissante des attaques menÃ©es par des agents basÃ©s sur de grands modÃ¨les de langage (LLM) pour la cybersÃ©curitÃ© moderne. Pour rÃ©pondre Ã  cette prÃ©occupation, les auteurs prÃ©sentent LLM Honeypot, un systÃ¨me conÃ§u pour surveiller les agents de piratage autonomes basÃ©s sur l'IA. En augmentant un honeypot SSH standard avec des techniques d'injection de prompts et d'analyse temporelle, le cadre de travail vise Ã  distinguer les agents LLM parmi tous les attaquants.

Au cours d'un dÃ©ploiement d'essai d'environ trois mois dans un environnement public, les auteurs ont recueilli 8 130 731 tentatives de piratage et identifiÃ© 8 agents potentiels de l'IA. Ce travail dÃ©montre l'Ã©mergence de menaces pilotÃ©es par l'IA et leur niveau d'utilisation actuel, servant d'alerte prÃ©coce concernant les agents malveillants LLM dans la nature.

**Lien vers l'article :** [http://arxiv.org/abs/2410.13919v2](http://arxiv.org/abs/2410.13919v2)

ğŸ“ State: Ready

Processing result 4...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

**RÃ©sumÃ© du document :**

L'article intitulÃ© "The Influence of Human-inspired Agentic Sophistication in LLM-driven Strategic Reasoners" examine l'impact de la sophistication agentique inspirÃ©e de l'humain sur les raisonneurs stratÃ©giques basÃ©s sur les grands modÃ¨les de langage (LLM). Avec l'essor rapide des LLM, la recherche en intelligence artificielle (IA) s'est tournÃ©e vers les systÃ¨mes agentiques, utilisant des notions plus faibles et plus flexibles d'agentivitÃ©. Cependant, cela soulÃ¨ve des questions clÃ©s sur la mesure dans laquelle les agents basÃ©s sur les LLM reproduisent le raisonnement stratÃ©gique humain, en particulier dans les contextes de thÃ©orie des jeux.

Les auteurs Ã©valuent trois conceptions d'agents : un modÃ¨le simple de thÃ©orie des jeux, un modÃ¨le LLM-as-agent non structurÃ©, et un LLM intÃ©grÃ© dans un cadre agentique traditionnel. En utilisant des jeux de devinettes comme banc d'essai, ils ont Ã©valuÃ© ces agents par rapport Ã  des participants humains en termes de motifs de raisonnement gÃ©nÃ©ral et d'objectifs spÃ©cifiques aux rÃ´les. De plus, ils ont introduit des scÃ©narios de jeu obscurcis pour Ã©valuer la capacitÃ© des agents Ã  gÃ©nÃ©raliser au-delÃ  des distributions d'entraÃ®nement.

L'analyse, couvrant plus de 2000 Ã©chantillons de raisonnement Ã  travers 25 configurations d'agents, montre que les structures cognitives inspirÃ©es de l'humain peuvent amÃ©liorer l'alignement des agents LLM avec le comportement stratÃ©gique humain. Cependant, la relation entre la complexitÃ© de la conception agentique et la similitude avec l'humain est non linÃ©aire, soulignant une dÃ©pendance critique aux capacitÃ©s sous-jacentes des LLM et suggÃ©rant des limites Ã  l'augmentation architecturale simple.

**Lien vers l'article :** [http://arxiv.org/abs/2505.09396v1](http://arxiv.org/abs/2505.09396v1)

ğŸ“ State: Ready

Processing result 5...

ğŸ“ State: Processing Queue

ğŸ“ State: Processing

ğŸ¤– Assistant:

**RÃ©sumÃ© du document :**

L'article intitulÃ© "BadAgent: Inserting and Activating Backdoor Attacks in LLM Agents" explore la vulnÃ©rabilitÃ© des agents intelligents basÃ©s sur de grands modÃ¨les de langage (LLM) aux attaques par porte dÃ©robÃ©e. Avec l'essor des LLM, des agents puissants ont Ã©tÃ© dÃ©veloppÃ©s pour fournir des services personnalisÃ©s avec des outils dÃ©finis par l'utilisateur. Les mÃ©thodes de pointe pour construire ces agents utilisent des LLM entraÃ®nÃ©s et les affinent davantage sur des donnÃ©es spÃ©cifiques Ã  la tÃ¢che de l'agent.

Cependant, les auteurs montrent que ces mÃ©thodes sont vulnÃ©rables Ã  des attaques par porte dÃ©robÃ©e qu'ils nomment BadAgent. Ces attaques peuvent Ãªtre intÃ©grÃ©es en affinant les agents sur des donnÃ©es de porte dÃ©robÃ©e. Ã€ l'heure du test, l'attaquant peut manipuler les agents LLM dÃ©ployÃ©s pour exÃ©cuter des opÃ©rations nuisibles en montrant le dÃ©clencheur dans l'entrÃ©e de l'agent ou dans l'environnement. De maniÃ¨re surprenante, les mÃ©thodes d'attaque proposÃ©es sont extrÃªmement robustes, mÃªme aprÃ¨s un affinement sur des donnÃ©es de confiance.

Bien que les attaques par porte dÃ©robÃ©e aient Ã©tÃ© largement Ã©tudiÃ©es dans le traitement du langage naturel, les auteurs affirment Ãªtre les premiers Ã  les Ã©tudier sur les agents LLM, qui sont plus dangereux en raison de la permission d'utiliser des outils externes. Ce travail dÃ©montre le risque clair de construire des agents LLM basÃ©s sur des LLM ou des donnÃ©es non fiables. Le code utilisÃ© dans cette Ã©tude est disponible publiquement Ã  l'adresse suivante : [https://github.com/DPamK/BadAgent](https://github.com/DPamK/BadAgent).

**Lien vers l'article :** [http://arxiv.org/abs/2406.03007v1](http://arxiv.org/abs/2406.03007v1)

ğŸ“ State: Ready

=== Demo Complete ===
